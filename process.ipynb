{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images on birds using whole training set (no human refinement)\n",
    "# Using Dense Neural Network\n",
    "\n",
    "from tensorflow.data import Dataset \n",
    "from tensorflow.image import decode_image\n",
    "from tensorflow import read_file\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the data set\n",
    "\n",
    "PATH = os.getcwd()\n",
    "\n",
    "# The ImageDataGenerator object assumes that resources will be preclassified, i.e., \n",
    "# resources/A will contain all images of \"A\" type\n",
    "# resources/B will contain all images of \"B\" type\n",
    "# The object \"classifies\" data based on the directory\n",
    "training_data_dir = os.path.join(PATH, 'scrape', 'resources')\n",
    "\n",
    "training_data_image_dir = os.path.join(training_data_dir, 'birds')\n",
    "\n",
    "training_data_size = len(os.listdir(training_data_image_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/franklin/Documents/dev/birdee/scrape/resources/birds\n"
     ]
    }
   ],
   "source": [
    "print(training_data_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "print(training_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(800, 533): 478, (896, 597): 322, (533, 800): 65, (1000, 771): 1, (1137, 794): 1, (969, 1000): 1, (467, 700): 2, (700, 467): 3, (900, 600): 57, (1000, 667): 3, (599, 800): 1, (597, 896): 35, (600, 900): 3, (800, 800): 4, (800, 556): 1, (800, 538): 1, (800, 400): 4, (1200, 600): 1, (800, 581): 1, (900, 651): 1, (800, 534): 1, (5184, 3456): 1, (800, 623): 1, (4894, 3263): 1, (799, 533): 3, (1000, 702): 1, (514, 800): 1, (900, 465): 1, (900, 795): 1, (1000, 755): 1, (1200, 538): 1}\n"
     ]
    }
   ],
   "source": [
    "# what are the sizes of the photographs?\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "sizes = {}\n",
    "for img in os.listdir(training_data_image_dir):\n",
    "    img_path = os.path.join(training_data_image_dir, img)\n",
    "    (width, height) = Image.open(img_path).size\n",
    "    if (width, height) in sizes:\n",
    "        sizes[(width, height)] = sizes[(width, height)] + 1\n",
    "    else:\n",
    "        sizes[(width, height)] = 1\n",
    "        \n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most images are in the 800 x 533 size\n",
    "# let's bump the size to 800 x 600 to be accomodating\n",
    "\n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_HEIGHT = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 998 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Prepare data - read images from disk\n",
    "\n",
    "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                                           directory=training_data_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/franklin/Documents/dev/birdee/env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/franklin/Documents/dev/birdee/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 600, 800, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 300, 400, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 300, 400, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 150, 200, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 150, 200, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 100, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 480000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               245760512 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 245,784,609\n",
      "Trainable params: 245,784,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "15/15 [==============================] - 209s 14s/step - loss: 0.0463 - acc: 1.0000\n",
      "Epoch 2/15\n",
      "15/15 [==============================] - 183s 12s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 3/15\n",
      "15/15 [==============================] - 193s 13s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 4/15\n",
      "15/15 [==============================] - 173s 12s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 5/15\n",
      "15/15 [==============================] - 179s 12s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 6/15\n",
      "15/15 [==============================] - 239s 16s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 7/15\n",
      "15/15 [==============================] - 226s 15s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 8/15\n",
      "15/15 [==============================] - 244s 16s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 9/15\n",
      "15/15 [==============================] - 227s 15s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 10/15\n",
      "15/15 [==============================] - 245s 16s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 11/15\n",
      "15/15 [==============================] - 218s 15s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "15/15 [==============================] - 273s 18s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "15/15 [==============================] - 251s 17s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "15/15 [==============================] - 298s 20s/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "15/15 [==============================] - 311s 21s/step - loss: 0.0000e+00 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15 \n",
    "\n",
    "# Train the model\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=training_data_size // BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    "    # validation_data=val_data_gen,\n",
    "    # validation_steps=total_val // BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.047489795235546035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
